{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminal Command Next Word Prediction\n",
    "\n",
    "This notebook demonstrates how to build a simple linear regression model to predict the next word in terminal commands. We'll go through the following steps:\n",
    "\n",
    "1. Data Generation and Exploration\n",
    "2. Data Preprocessing\n",
    "3. Feature Engineering\n",
    "4. Model Training\n",
    "5. Evaluation and Prediction\n",
    "\n",
    "## Setup\n",
    "First, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Exploration\n",
    "\n",
    "We'll create a synthetic dataset of common terminal commands and analyze its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_terminal_commands():\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset of terminal commands.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of terminal command strings\n",
    "    \"\"\"\n",
    "    commands = [\n",
    "        \"ls -la /home\",\n",
    "        \"cd /usr/local/bin\",\n",
    "        \"cp file1.txt file2.txt\",\n",
    "        \"mv document.pdf Downloads\",\n",
    "        \"rm temp.txt\",\n",
    "        \"mkdir new_directory\",\n",
    "        \"touch newfile.txt\",\n",
    "        \"chmod 755 script.sh\",\n",
    "        \"grep pattern file.txt\",\n",
    "        \"find . -name *.py\",\n",
    "        \"ps aux | grep process\",\n",
    "        \"df -h\",\n",
    "        \"pwd\",\n",
    "        \"tar -czf archive.tar.gz files\",\n",
    "        \"wget https://example.com/file\"\n",
    "    ] * 10\n",
    "    return commands\n",
    "\n",
    "# Generate commands\n",
    "commands = generate_terminal_commands()\n",
    "\n",
    "# Display first few commands\n",
    "print(\"Sample commands:\")\n",
    "for cmd in commands[:5]:\n",
    "    print(f\"- {cmd}\")\n",
    "\n",
    "# Analyze command lengths\n",
    "cmd_lengths = [len(cmd.split()) for cmd in commands]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(cmd_lengths, bins=range(min(cmd_lengths), max(cmd_lengths) + 2, 1))\n",
    "plt.title('Distribution of Command Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Now we'll prepare the data for our model by splitting commands into sequences and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_commands(commands):\n",
    "    \"\"\"\n",
    "    Preprocess commands into input sequences and target words.\n",
    "    \n",
    "    Args:\n",
    "        commands (list): List of command strings\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) where X is list of input sequences and y is list of target words\n",
    "    \"\"\"\n",
    "    # Split commands into words\n",
    "    command_sequences = [cmd.split() for cmd in commands]\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for sequence in command_sequences:\n",
    "        if len(sequence) > 1:\n",
    "            input_seq = sequence[:-1]\n",
    "            padded_seq = input_seq + ['PAD'] * (5 - len(input_seq))\n",
    "            X.append(padded_seq)\n",
    "            y.append(sequence[-1])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = preprocess_commands(commands)\n",
    "\n",
    "print(\"Example preprocessing:\")\n",
    "for i in range(3):\n",
    "    print(f\"Input sequence: {X[i]}\")\n",
    "    print(f\"Target word: {y[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We'll convert our text data into numerical features using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def encode_features(X, y):\n",
    "    \"\"\"\n",
    "    Convert text features to numerical using one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        X (list): List of input sequences\n",
    "        y (list): List of target words\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_encoded, y_encoded, encoder_X, encoder_y)\n",
    "    \"\"\"\n",
    "    encoder_X = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    encoder_y = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "    \n",
    "    X_flat = np.array(X).reshape(-1, 1)\n",
    "    encoder_X.fit(X_flat)\n",
    "    \n",
    "    X_encoded = []\n",
    "    for sequence in X:\n",
    "        sequence_encoded = encoder_X.transform(np.array(sequence).reshape(-1, 1))\n",
    "        X_encoded.append(sequence_encoded.flatten())\n",
    "    \n",
    "    y_encoded = encoder_y.fit_transform(np.array(y).reshape(-1, 1))\n",
    "    \n",
    "    return np.array(X_encoded), y_encoded, encoder_X, encoder_y\n",
    "\n",
    "X_encoded, y_encoded, encoder_X, encoder_y = encode_features(X, y)\n",
    "\n",
    "print(\"Feature encoding shapes:\")\n",
    "print(f\"X shape: {X_encoded.shape}\")\n",
    "print(f\"y shape: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Now we'll train our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X (numpy.ndarray): Encoded input features\n",
    "        y (numpy.ndarray): Encoded target values\n",
    "        \n",
    "    Returns:\n",
    "        LinearRegression: Trained model\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = train_model(X_train, y_train)\n",
    "\n",
    "# Calculate training and test scores\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Training R² score: {train_score:.4f}\")\n",
    "print(f\"Testing R² score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Prediction\n",
    "\n",
    "Let's test our model with some example predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_next_word(model, input_sequence, encoder_X, encoder_y):\n",
    "    \"\"\"\n",
    "    Predict the next word given an input sequence.\n",
    "    \n",
    "    Args:\n",
    "        model (LinearRegression): Trained model\n",
    "        input_sequence (list): List of input words\n",
    "        encoder_X (OneHotEncoder): Encoder for input features\n",
    "        encoder_y (OneHotEncoder): Encoder for target values\n",
    "        \n",
    "    Returns:\n",
    "        str: Predicted next word\n",
    "    \"\"\"\n",
    "    padded_seq = input_sequence + ['PAD'] * (5 - len(input_sequence))\n",
    "    \n",
    "    sequence_encoded = []\n",
    "    for word in padded_seq:\n",
    "        word_encoded = encoder_X.transform(np.array([word]).reshape(-1, 1))\n",
    "        sequence_encoded.extend(word_encoded.flatten())\n",
    "    \n",
    "    prediction = model.predict([sequence_encoded])\n",
    "    predicted_word = encoder_y.inverse_transform(prediction.reshape(1, -1))[0][0]\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Test some predictions\n",
    "test_sequences = [\n",
    "    ['ls', '-la'],\n",
    "    ['cd', '/usr'],\n",
    "    ['grep', 'pattern'],\n",
    "    ['mkdir']\n",
    "]\n",
    "\n",
    "print(\"Example predictions:\")\n",
    "for seq in test_sequences:\n",
    "    predicted = predict_next_word(model, seq, encoder_X, encoder_y)\n",
    "    print(f\"Input: {' '.join(seq)}\")\n",
    "    print(f\"Predicted next word: {predicted}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Limitations and Potential Improvements\n",
    "\n",
    "1. **Dataset Limitations**:\n",
    "   - Small synthetic dataset\n",
    "   - Limited variety of commands\n",
    "   - No real-world usage patterns\n",
    "\n",
    "2. **Model Limitations**:\n",
    "   - Linear regression might not capture complex patterns\n",
    "   - No consideration of word order importance\n",
    "   - Limited context window\n",
    "\n",
    "3. **Potential Improvements**:\n",
    "   - Use a larger, real-world dataset\n",
    "   - Implement more sophisticated models (LSTM, Transformer)\n",
    "   - Add context-aware features\n",
    "   - Include command history\n",
    "   - Add error handling for unknown words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}